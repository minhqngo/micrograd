{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46142cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff65704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from micrograd.loss import CrossEntropyLoss\n",
    "from micrograd.engine import Value\n",
    "from micrograd.nn import MLP\n",
    "from micrograd.optimizer import SGD\n",
    "from micrograd.functional import softmax\n",
    "from micrograd.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68344bb4",
   "metadata": {},
   "source": [
    "# Data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_val_data(data_root, mode='train'):\n",
    "    assert mode.lower() in ['train', 'val'], f\"Mode must be in {['train', 'val']}\"\n",
    "    data = pd.read_csv(os.path.join(data_root, 'train.csv'))\n",
    "    train, val = train_test_split(data, random_state=42, test_size=0.2)\n",
    "    if mode == 'train':\n",
    "        data = train\n",
    "    else:\n",
    "        data = val\n",
    "    images = data.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
    "    images /= 255.\n",
    "    labels = data.iloc[:, 0].to_numpy(dtype=np.int32)\n",
    "    return [(image, label) for image, label in zip(images, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9231133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_root):\n",
    "    data = pd.read_csv(os.path.join(data_root, 'test.csv'))\n",
    "    images = data.iloc[:].to_numpy(dtype=np.float32)\n",
    "    images /= 255.\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8afc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(inputs, labels):\n",
    "    assert len(inputs) == len(labels), \"Number of input samples must match number of labels\"\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    inputs, labels = inputs[indices], labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cc1ec",
   "metadata": {},
   "source": [
    "# Misc utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bcf363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_label(probs):\n",
    "    labels = np.argmax(probs, axis=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(label, pred):\n",
    "    return np.mean(label == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5279685",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748387fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/minh/datasets/MNIST-kaggle/'\n",
    "LOG_ROOT = './logs/kaggle/softmax'\n",
    "WEIGHTS_PATH = \"mnist_mlp_kaggle.npz\"\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = load_train_val_data(DATA_ROOT, mode='train')\n",
    "val_ds = load_train_val_data(DATA_ROOT, mode='val')\n",
    "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = MLP(nin=784, nouts=[32, 16, 10])\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), learning_rate=LEARNING_RATE)\n",
    "\n",
    "logs = {\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'train_loss': []\n",
    "}\n",
    "\n",
    "best_val_acc = float('-inf')\n",
    "best_model = None\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc = 0.\n",
    "    train_loss = 0.\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = Value(inputs)\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        train_loss += loss.data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_probs = softmax(logits)\n",
    "        train_preds = np.argmax(train_probs, axis=1)\n",
    "        train_acc += np.mean(labels == train_preds)\n",
    "        \n",
    "        logs['train_acc'].append(np.mean(labels == train_preds))\n",
    "        logs['train_loss'].append(loss.data)\n",
    "        \n",
    "    train_acc /= len(train_loader)\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch: {epoch + 1}. Train loss={train_loss}. Train acc={train_acc * 100:.2f}%\")\n",
    "    \n",
    "    val_acc = 0.\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = Value(inputs)\n",
    "        \n",
    "        logits = model(inputs)            \n",
    "        val_probs = softmax(logits)\n",
    "        val_preds = np.argmax(val_probs, axis=1)\n",
    "        val_acc += np.mean(labels == val_preds)\n",
    "        \n",
    "        logs['val_acc'].append(np.mean(labels == val_preds))\n",
    "        \n",
    "    val_acc /= len(val_loader)\n",
    "    print(f\"Epoch: {epoch + 1}. Val acc={val_acc * 100:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "best_model.save_weights(WEIGHTS_PATH)\n",
    "\n",
    "os.makedirs(LOG_ROOT, exist_ok=True)\n",
    "plt.plot(logs['train_acc'], label='Train accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.title('Training accuracy graph')\n",
    "plt.savefig(os.path.join(LOG_ROOT, 'train_accuracy.jpg'))\n",
    "plt.clf()\n",
    "plt.plot(logs['val_acc'], label='Val accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.title('Val accuracy graph')\n",
    "plt.savefig(os.path.join(LOG_ROOT, 'val_accuracy.jpg'))\n",
    "plt.clf()\n",
    "plt.plot(logs['train_loss'], label='Train losses')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.title('Training loss graph')\n",
    "plt.savefig(os.path.join(LOG_ROOT, 'loss.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df7e7",
   "metadata": {},
   "source": [
    "# Run on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/home/minh/datasets/MNIST-kaggle/'\n",
    "model = MLP(nin=784, nouts=[32, 16, 10])\n",
    "model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "test_images = load_test_data(DATA_ROOT)\n",
    "test_images = Value(test_images)\n",
    "test_outputs = model(test_images)\n",
    "test_preds = probs_to_label(test_outputs.data)\n",
    "test_preds = [(i + 1, v) for i, v in enumerate(test_preds)]\n",
    "test_preds = pd.DataFrame(test_preds, columns=['ImageID', 'Label'])\n",
    "test_preds.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_np_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
